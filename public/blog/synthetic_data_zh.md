---
title: "大白话讲解合成数据"
date: "2024-10-22"
excerpt: "用通俗易懂的语言解释了什么是合成数据、为什么需要它，以及如何使用它。通过生动的比喻和日常生活中的例子，让读者轻松理解这个看似复杂的技术概念。无论你是技术专家还是初学者，都能从这篇指南中获得有价值的见解。"
coverImage: "blog-02.jpg"
author:
  name: "Jessy Tsui"
  avatar: "panda.jpg"
keywords:
  - LLM Synthetic Data
  - LLM Data Engineering
---
# 前言
中文互联网关于AI最前沿的研究资料属实较少，而这点在行业最前端的投资、创业圈尤甚。

我们会发布一些和合成数据、数据工程有关的科普和思考类的文章，本篇试图拆解合成数据爆火的原因，同时以大白话的语言描述这一领域是做什么的。之后会更新一些更深的思考

<img src="https://picx.zhimg.com/80/v2-c7cd74c4bddab0a074a1301c73da1873_720w.webp" style="zoom:150%;" />

## 训练范式的迁移
总有很多人在试图寻找“SFT和RLHF最佳配比”，他们大多只能得到InstructGPT 、WebGPT 、Sparrow 、Helpful和Harmless Assistant这几篇已经有些过时的工作。

以 Llama 3.1 为界限，前LLM时代有过训练经验的团队都知道“**10k个高质量指令和100k个偏好数据**”的经验法则。

这些开创性的工作在ChatGPT伊始贡献了很多内容，可以让研究领域快速追赶OpenAI的进程。但这些作品已经过时了，与今天 RLHF 的训练方式不符。

尽管过去的评估、训练目标可能仍是一致的，但在细节方面已经有了翻天覆地的差距。

Llama 3.1 论文包含有关其post-training过程的大量细节。而随后 Nvidia 的 Nemotron 340B、Apple Intelligence、Gemma 2的模型报告都已明确表明：存在一种用于更高级的更先进的 RLHF 的新型训练方式。而这种方式依赖于一些实验性质的假设：

1. **合成数据的质量比人类真实数据更高**，特别是对于具有挑战性的任务而言，比如数学推理、逻辑判断。

2. SFT 和 RLHF 所需的**数据数量远大于前LLM时代的“10k个高质量指令和100k个偏好数据”**的经验理论。

3. 需要**多轮训练和生成**才能达到最佳模型。

4. **数据过滤和清洗**是训练中最重要的部分。

<img src="https://pic3.zhimg.com/80/v2-c949ee5a3350ac2440540ab9eb6370d6_720w.webp" style="zoom:150%;" />

## 模型崩溃被有意亏大

七月份被很多媒体盛传的这两篇Nature论文说道：使用过多的合成数据会导致模型崩溃

<img src="https://pic1.zhimg.com/80/v2-0e84161a8aa56850a540c7a10766a9f8_720w.webp" style="zoom:150%;" />

<img src="https://pic1.zhimg.com/80/v2-e0814ded4c0aadc3f1cdfdaed9028866_720w.webp" style="zoom:150%;" />


### 真实情况是什么？

**研究人员故意以与实际情况不符的条件进行实验**

如何理解？

模型崩溃来自这样一个问题：`将之前前生成模型的合成数据，用于训练新生成模型的预训练时会发生什么？`

<img src="https://picx.zhimg.com/80/v2-ce4e0be88e6eb4fa5dedf8dca0726fe1_720w.webp" style="zoom:150%;" />

而这篇Nature的实验条件设置为：

- 每次迭代后丢弃所有数据

- 使用固定的数据集大小

都和真实场景不符

<img src="https://pic3.zhimg.com/80/v2-21b8c31fbc2a3a386f53a9517845baa8_720w.webp" style="zoom:150%;" />

他们还做了一组对比实验，也与真实场景不符：

保持数据集大小恒定，保留 10% 的原始数据，但替换了其他的 90%。

PS: 但尽管如此，也可以看到，通过添加一些真实数据，实验已经看到了更低的困惑度

<img src="https://pica.zhimg.com/80/v2-01962750d23dc254e3087f557af83986_720w.webp" style="zoom:150%;" />

如果我们保留全部的数据，每次只是新增加数据会怎样？可以看这篇COLM

如果这样做，即数据会积累（右图），模型不会崩溃✅

如果不这样做，即数据会被替换（左图），模型崩溃❌

<img src="https://picx.zhimg.com/80/v2-9a9322b57a09f88b90ebdc7922649055_720w.webp" style="zoom:150%;" />

而这一结果在被证明在了各个领域（文本、视觉、高分子）和模型（transformer、VAE、diffusion）上

<img src="https://pica.zhimg.com/80/v2-262d3530fbdabd174de8f68e49fb2240_720w.webp" style="zoom:150%;" />

<img src="https://pic2.zhimg.com/80/v2-2190b5f864a8c9f15f6fdae7e1664447_720w.webp" style="zoom:150%;" />


# 正文

## 认识合成数据 - "AI的营养配方师"

### 1.1 什么是合成数据？

在人工智能快速发展的今天，"数据是AI的粮食"这句话可谓一语中的。就像人类需要营养均衡的食物来保持健康一样，AI系统也需要优质、丰富的数据来提升性能。但是，优质数据就像米其林三星餐厅的食材一样，往往获取困难、成本高昂。这时候，"合成数据"就像一位能够完美复制美食的营养配方师，为AI提供了取之不竭的"营养来源"。

### 1.2 AI的"营养需求"

想象一下，如果你要开一家餐厅，你需要：

- 🥬 新鲜的食材（高质量原始数据）
- 🍱 多样的菜品（不同类型的数据）
- 📦 可靠的供应链（稳定的数据来源）
- 💰 合理的成本（数据获取成本）

但在现实中，这些需求常常难以同时满足。就像高档食材可能供应稀缺、价格昂贵一样，高质量的数据也面临着类似的困境。

### 1.3 合成数据的能力

这时候，合成数据就像一位神奇的营养配方师，他能够：

1. 🧪 **精准配方**：根据算法创造所需的"食材"（数据）
2. 🔄 **营养均衡**：确保数据的多样性和平衡性
3. ⚡ **灵活调整**：随时根据需求修改生成策略
4. 🚀 **批量生产**：快速生成大规模数据集

### 1.4 "烹饪"合成数据的步骤

## 1.4.1 选择"食材"（定义需求）

```
示例：电影评论生成需求
- 数量：1000条评论
- 分布：正面500条，负面500条
- 长度：每条50-100字
- 要素：包含情感、剧情、演技评价
```

## 1.4.2 准备"配方"（设计提示词）

```
任务：生成影评
电影：《盗梦空间》
要求：
- 评价要有理有据
- 必须提及剧情和特效
- 包含个人观影感受
```

## 1.4.3 数据生成示例

```
生成结果：
"《盗梦空间》是一部视觉与思维的双重盛宴。
诺兰的叙事手法令人着迷，层层递进的梦境
结构让观众沉浸其中。特效制作精良，尤其是
那个标志性的城市折叠场景，至今难忘。"
```

### 1.5 合成数据的特点

## 1.5.1 优势

1. 📊 品质可控
   - 精确控制数据特征
   - 保证样本均衡性
2. 🚀 高效生产
   - 快速生成大量数据
   - 成本显著降低
3. 🎯 定制灵活
   - 场景自由组合
   - 按需即时生成
4. 🔒 隐私安全
   - 不涉及真实数据
   - 降低泄露风险

## 1.5.2 局限性

1. 🤔 真实度挑战
   - 可能缺乏真实感
   - 细节可能不够自然
2. ⚠️ 需要验证
   - 质量检查必不可少
   - 可能需要人工审核

### 1.6 应用案例展示

## 1.6.1 医疗诊断数据

```
患者病例：
姓名：张**（已脱敏）
年龄：65岁
主诉：持续性头痛，轻微眩晕
检查结果：
- 血压：145/90 mmHg
- 心率：78次/分
- 血糖：6.2 mmol/L
家族史：父亲有高血压病史
```

## 1.6.2 金融风控数据

```
交易记录：
时间：2024-01-15 03:21:15
交易金额：¥8,888.00
交易地点：异地（距常用地点1500km）
交易特征：
- 非常用时间段
- 连续多笔大额交易
- 异常地理位置
```

## 1.6.3 客服对话数据

```
客户反馈：
"您好，我昨天购买的手机今天开机后发现有异响，
而且电池续航只有3小时，这明显不符合商品描述，
希望能尽快处理这个问题。"

客服回复：
"您好，非常抱歉给您带来不便。请问方便提供下
订单号和机器型号吗？我们会优先处理您的问题，
并安排专业技术人员进行检测维修。"
```

## 第二部分：合成数据技术核心概念解析

### 2.1 数据增强（Data Augmentation）

就像厨师会用不同的烹饪方法让一道菜变出多种口味，数据增强就是让一份数据产生多个变体的技术。

## 2.1.1 什么是数据增强？

数据增强就是通过各种转换方法，从现有数据生成更多样化的数据。就像：

- 把"这部电影很好看"变成：
  - "这部电影非常精彩"
  - "这部影片令人印象深刻"
  - "这部作品十分出色"

## 2.1.2 常见的增强方法

```
1. 同义词替换
   原句："这手机的电池很耐用"
   变体："这部手机的续航很出色"

2. 句式重组
   原句："手机很漂亮，性能也不错"
   变体："不仅性能优秀，外观也很出众"

3. 内容扩展
   原句："这部电影很感人"
   变体："这部电影情节感人，特别是结尾处父子重逢的场景让人潸然泪下"
```

### 2.2 数据监测（Data Monitoring）

就像医生需要监测病人的各项指标一样，数据监测是对生成数据进行实时"体检"。

## 2.2.1 监测内容

```
主要监测指标：
1. 数据完整性
   - 是否缺失关键信息
   - 格式是否规范

2. 数据准确性
   - 事实是否正确
   - 逻辑是否合理

3. 数据多样性
   - 内容是否单一
   - 表达是否重复
```

## 2.2.2 监测方法

```
监测手段：
✅ 自动化检查
   - 格式检验
   - 规则匹配
   - 统计分析

🔍 人工抽查
   - 质量评估
   - 内容审核
   - 专业验证
```

### 2.3 数据质量控制（Quality Control）

就像食品安全检测一样，确保生成的数据符合使用标准。

## 2.3.1 质量维度

```
维度说明检查方法
准确性内容是否准确事实核查
完整性信息是否完整字段检查
一致性逻辑是否统一规则验证
时效性信息是否过时时间检查
```

## 2.3.2 控制方法

```
1. 预防控制
   - 设定生成规则
   - 制定质量标准
   - 优化生成模板

2. 过程控制
   - 实时监测
   - 及时纠正
   - 动态调整

3. 结果控制
   - 样本检查
   - 质量评估
   - 反馈优化
```

### 2.4 多步骤生成（Multi-step Generation）

就像制作一道复杂的菜品需要多个步骤，复杂的数据也需要分步骤生成。

## 2.4.1 基本流程

```
第一步：框架生成
🏗️ 确定主要结构
例如：问题-回答的基本框架

第二步：内容填充
📝 添加具体内容
例如：详细的问题描述和解答步骤

第三步：细节优化
✨ 补充细节和修饰
例如：添加专业术语、调整语言风格

第四步：质量提升
🔍 优化和完善
例如：检查逻辑性、增加例证
```

## 2.4.2 应用示例

```
生成客服对话：

1️⃣ 框架确定：
客户：[问题描述]
客服：[初步回应]
客户：[追问]
客服：[解决方案]

2️⃣ 内容填充：
客户："我的手机突然无法开机了"
客服："您好，请问手机是什么型号？"
客户："iPhone 13"
客服："好的，让我们先试试基本的处理方法"

3️⃣ 细节补充：
客户："我的iPhone 13突然黑屏无法开机了，之前没有任何异常"
客服："您好，非常抱歉给您带来困扰。请问手机是否有进水或摔落过？"
客户："没有，使用时一直很小心"
客服："好的，让我们先尝试强制重启：同时按住音量加键和电源键10秒..."

4️⃣ 优化完善：
[添加更多技术细节]
[补充可能的原因分析]
[加入解决方案的成功率说明]
```

### 2.5 数据合成管道（Data Synthesis Pipeline）

像工厂的生产线一样，将各个处理环节有序连接。

## 2.5.1 管道组成

```
  A[需求分析] --> B[数据设计]
  B --> C[初始生成]
  C --> D[质量检测]
  D --> E[数据增强]
  E --> F[最终验收]
```

## 2.5.2 关键环节

```
1. 输入层
   - 需求确认
   - 参数设置
   - 模板准备

2. 处理层
   - 数据生成
   - 质量控制
   - 格式转换

3. 输出层
   - 结果验证
   - 数据存储
   - 使用分发
```

这些概念是构建高质量合成数据的基础，理解它们有助于我们更好地应用合成数据技术。就像制作美食需要掌握各种烹饪技巧一样，生成好的合成数据也需要熟练运用这些技术工具。